"""
ğŸš€ GOOGLE COLAB - DERÄ°N TARAMA + TÃœM VERÄ°LERLE EÄÄ°TÄ°M
=====================================================
241K+ DOSYAYI TARAR - TÃœM ALT KLASÃ–RLERE GÄ°RER - HEPSÄ°NÄ° KULLANIR!
"""

print("=" * 80)
print("ğŸ”¥ NEURALCIPHER - DERÄ°N TARAMA VE MAKSIMUM VERÄ° EÄÄ°TÄ°MÄ°")
print("=" * 80)

# ============================================================
# ADIM 1: KURULUM
# ============================================================
print("\nğŸ”§ KÃ¼tÃ¼phaneler yÃ¼kleniyor...")
import subprocess
import sys

try:
    import librosa
except:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "librosa", "scipy"])
    print("âœ… librosa yÃ¼klendi")

# ============================================================
# ADIM 2: GOOGLE DRIVE MOUNT
# ============================================================
print("\nğŸ“‚ Google Drive baÄŸlanÄ±yor...")
try:
    from google.colab import drive
    import os
    if not os.path.exists('/content/drive'):
        drive.mount('/content/drive')
    print("âœ… Drive baÄŸlandÄ±")
except:
    print("âš ï¸ Colab dÄ±ÅŸÄ±nda Ã§alÄ±ÅŸÄ±yor")

# ============================================================
# ADIM 3: KÃœTÃœPHANELER
# ============================================================
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import glob
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# ADIM 4: KONFÄ°GÃœRASYON
# ============================================================
class Config:
    DATA_PATH = "/content/drive/MyDrive/Veriler/"
    OUTPUT_PATH = "/content/drive/MyDrive/NeuralCipher_Output/"
    
    # Tarama ayarlarÄ±
    MAX_FILES_TO_SCAN = 50000  # Ä°lk 50K dosyayÄ± tara
    MAX_FILES_TO_LOAD = 100    # Ä°lk 100 kullanÄ±labilir dosyayÄ± yÃ¼kle
    
    # Model ayarlarÄ±
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    RANDOM_STATE = 42
    
    # Dosya tipleri
    CSV_EXTENSIONS = ['.csv', '.data', '.txt']
    AUDIO_EXTENSIONS = ['.wav', '.mp3', '.m4a', '.flac']
    MATLAB_EXTENSIONS = ['.mat']
    
config = Config()
os.makedirs(config.OUTPUT_PATH, exist_ok=True)

print(f"\nâš™ï¸ KonfigÃ¼rasyon:")
print(f"ğŸ“‚ Veri Yolu: {config.DATA_PATH}")
print(f"ğŸ’¾ Ã‡Ä±ktÄ± Yolu: {config.OUTPUT_PATH}")
print(f"ğŸ” Maksimum Tarama: {config.MAX_FILES_TO_SCAN:,} dosya")
print(f"ğŸ“¥ Maksimum YÃ¼kleme: {config.MAX_FILES_TO_LOAD} dosya")
print(f"ğŸ”„ Epoch: {config.EPOCHS}")

# GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    print(f"âœ… GPU: {len(gpus)} cihaz")
else:
    print("âš ï¸ GPU yok, CPU kullanÄ±lÄ±yor")

# ============================================================
# ADIM 5: DERÄ°N TARAMA SÄ°STEMÄ°
# ============================================================
class DeepScanner:
    """241K+ dosyayÄ± tarar, tÃ¼m alt klasÃ¶rlere girer"""
    
    def __init__(self, root_path):
        self.root_path = root_path
        self.stats = {
            'total_scanned': 0,
            'csv_found': 0,
            'audio_found': 0,
            'matlab_found': 0,
            'other_found': 0
        }
    
    def scan_all_files(self, max_files=50000):
        """TÃ¼m dosyalarÄ± recursive olarak tara"""
        print("\n" + "=" * 80)
        print("ğŸ” DERÄ°N TARAMA BAÅLIYOR")
        print("=" * 80)
        print(f"ğŸ“‚ KÃ¶k Dizin: {self.root_path}")
        print(f"ğŸ¯ Maksimum: {max_files:,} dosya taranacak")
        print("\nâ³ Tarama devam ediyor (bu 5-10 dakika sÃ¼rebilir)...\n")
        
        all_files = {
            'csv': [],
            'audio': [],
            'matlab': [],
            'other': []
        }
        
        try:
            # os.walk ile TÃœM alt klasÃ¶rlere gir
            for root, dirs, files in os.walk(self.root_path):
                for file in files:
                    if self.stats['total_scanned'] >= max_files:
                        break
                    
                    self.stats['total_scanned'] += 1
                    
                    # Her 1000 dosyada bir rapor
                    if self.stats['total_scanned'] % 1000 == 0:
                        print(f"ğŸ“Š Taranan: {self.stats['total_scanned']:,} | "
                              f"CSV: {self.stats['csv_found']} | "
                              f"Audio: {self.stats['audio_found']} | "
                              f"MATLAB: {self.stats['matlab_found']}")
                    
                    file_path = os.path.join(root, file)
                    file_ext = os.path.splitext(file)[1].lower()
                    
                    # CSV/TXT/DATA dosyalarÄ±
                    if file_ext in config.CSV_EXTENSIONS or file.endswith('.data'):
                        all_files['csv'].append(file_path)
                        self.stats['csv_found'] += 1
                    
                    # Audio dosyalarÄ±
                    elif file_ext in config.AUDIO_EXTENSIONS:
                        all_files['audio'].append(file_path)
                        self.stats['audio_found'] += 1
                    
                    # MATLAB dosyalarÄ±
                    elif file_ext in config.MATLAB_EXTENSIONS:
                        all_files['matlab'].append(file_path)
                        self.stats['matlab_found'] += 1
                    
                    else:
                        self.stats['other_found'] += 1
                
                if self.stats['total_scanned'] >= max_files:
                    break
        
        except Exception as e:
            print(f"âš ï¸ Tarama hatasÄ±: {e}")
        
        print("\n" + "=" * 80)
        print("âœ… TARAMA TAMAMLANDI")
        print("=" * 80)
        print(f"ğŸ“Š Toplam Taranan: {self.stats['total_scanned']:,} dosya")
        print(f"ğŸ“„ CSV/TXT/DATA: {self.stats['csv_found']} dosya")
        print(f"ğŸµ Audio: {self.stats['audio_found']} dosya")
        print(f"ğŸ”¬ MATLAB: {self.stats['matlab_found']} dosya")
        print(f"ğŸ“¦ DiÄŸer: {self.stats['other_found']} dosya")
        
        return all_files

# ============================================================
# ADIM 6: VERÄ° YÃœKLEYICI
# ============================================================
class DataLoader:
    """Bulunan dosyalarÄ± yÃ¼kler ve birleÅŸtirir"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.loaded_count = 0
    
    def load_csv_file(self, file_path):
        """Tek bir CSV dosyasÄ±nÄ± yÃ¼kle"""
        try:
            # FarklÄ± encoding'leri dene
            for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    if len(df) > 0 and len(df.columns) > 1:
                        return df
                except:
                    continue
            return None
        except:
            return None
    
    def load_multiple_csv(self, csv_files, max_files=100):
        """Birden fazla CSV dosyasÄ±nÄ± yÃ¼kle ve birleÅŸtir"""
        print("\n" + "=" * 80)
        print("ğŸ“¥ VERÄ° YÃœKLEME BAÅLIYOR")
        print("=" * 80)
        print(f"ğŸ¯ Maksimum: {max_files} dosya yÃ¼klenecek")
        
        all_dataframes = []
        loaded = 0
        
        # Ã–ncelikli dosya isimleri
        priority_keywords = [
            'parkinson', 'updrs', 'telemonitoring',
            'sample', 'dataset', 'data', 'train', 'test'
        ]
        
        # Ã–nce Ã¶ncelikli dosyalarÄ± yÃ¼kle
        priority_files = []
        other_files = []
        
        for f in csv_files:
            fname = os.path.basename(f).lower()
            if any(kw in fname for kw in priority_keywords):
                priority_files.append(f)
            else:
                other_files.append(f)
        
        # Ã–ncelikli + diÄŸer dosyalarÄ± birleÅŸtir
        sorted_files = priority_files + other_files
        
        for file_path in sorted_files[:max_files]:
            if loaded >= max_files:
                break
            
            df = self.load_csv_file(file_path)
            if df is not None:
                all_dataframes.append(df)
                loaded += 1
                fname = os.path.basename(file_path)
                print(f"âœ… [{loaded}/{max_files}] {fname}: {len(df)} satÄ±r, {len(df.columns)} sÃ¼tun")
        
        if all_dataframes:
            print(f"\nğŸ”„ {len(all_dataframes)} dosya birleÅŸtiriliyor...")
            combined = pd.concat(all_dataframes, ignore_index=True)
            print(f"âœ… Toplam: {len(combined)} satÄ±r, {len(combined.columns)} sÃ¼tun")
            self.loaded_count = loaded
            return combined
        
        return None
    
    def prepare_data(self, df):
        """Veriyi eÄŸitime hazÄ±rla"""
        print("\n" + "=" * 80)
        print("ğŸ”§ VERÄ° HAZIRLANIYOR")
        print("=" * 80)
        
        # Status/label sÃ¼tununu bul
        status_col = None
        possible_labels = ['status', 'Status', 'label', 'Label', 'class', 'Class', 
                          'target', 'Target', 'diagnosis', 'Diagnosis']
        
        for col in possible_labels:
            if col in df.columns:
                status_col = col
                break
        
        if status_col is None:
            print("âš ï¸ Label sÃ¼tunu bulunamadÄ±, ilk sÃ¼tun label olarak kullanÄ±lacak")
            status_col = df.columns[0]
        
        print(f"ğŸ·ï¸ Label sÃ¼tunu: {status_col}")
        
        # Gereksiz sÃ¼tunlarÄ± Ã§Ä±kar
        drop_cols = ['name', 'Name', 'id', 'ID', 'subject', 'Subject', 'patient', 'Patient']
        df = df.drop([c for c in drop_cols if c in df.columns], axis=1, errors='ignore')
        
        # X ve y'yi ayÄ±r
        y = df[status_col].values
        X = df.drop([status_col], axis=1, errors='ignore')
        
        # Sadece numerik sÃ¼tunlarÄ± al
        X = X.select_dtypes(include=[np.number]).values
        
        # Label'Ä± binary yap
        if len(np.unique(y)) > 2:
            # EÄŸer Ã§ok sÄ±nÄ±f varsa, ortalamanÄ±n Ã¼stÃ¼/altÄ± olarak ikiye bÃ¶l
            y = (y > np.median(y)).astype(int)
        else:
            y = (y != 0).astype(int)
        
        print(f"ğŸ“Š X shape: {X.shape}")
        print(f"ğŸ“Š y shape: {y.shape}")
        print(f"ğŸ·ï¸ PD: {np.sum(y)}, HC: {len(y) - np.sum(y)}")
        
        # Normalize
        X = self.scaler.fit_transform(X)
        
        return X.astype(np.float32), y.astype(np.float32)
    
    def create_sample_data(self, n=1000):
        """Yedek sample data"""
        print("\nâš ï¸ GerÃ§ek veri bulunamadÄ±, sample data oluÅŸturuluyor...")
        X = np.random.randn(n, 22).astype(np.float32)
        y = np.random.randint(0, 2, n).astype(np.float32)
        return X, y

# ============================================================
# ADIM 7: MODEL
# ============================================================
def build_model(input_dim):
    """GeliÅŸmiÅŸ neural network"""
    inputs = Input(shape=(input_dim,))
    
    x = Dense(256, activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    outputs = Dense(1, activation='sigmoid')(x)
    
    return models.Model(inputs=inputs, outputs=outputs)

# ============================================================
# ADIM 8: ANA EÄÄ°TÄ°M
# ============================================================
def main():
    print("\n" + "=" * 80)
    print("ğŸš€ EÄÄ°TÄ°M BAÅLIYOR")
    print("=" * 80)
    
    # 1. DERÄ°N TARAMA
    scanner = DeepScanner(config.DATA_PATH)
    all_files = scanner.scan_all_files(max_files=config.MAX_FILES_TO_SCAN)
    
    # 2. VERÄ° YÃœKLEME
    loader = DataLoader()
    
    if all_files['csv']:
        print(f"\nâœ… {len(all_files['csv'])} CSV dosyasÄ± bulundu!")
        combined_df = loader.load_multiple_csv(all_files['csv'], max_files=config.MAX_FILES_TO_LOAD)
        
        if combined_df is not None and len(combined_df) > 100:
            X, y = loader.prepare_data(combined_df)
        else:
            print("âš ï¸ Yeterli veri yÃ¼klenemedi")
            X, y = loader.create_sample_data()
    else:
        print("âš ï¸ HiÃ§ CSV dosyasÄ± bulunamadÄ±")
        X, y = loader.create_sample_data()
    
    # 3. VERÄ° BÃ–LME
    print("\n" + "=" * 80)
    print("âœ‚ï¸ VERÄ° BÃ–LÃœNÃœYOR")
    print("=" * 80)
    
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, stratify=y, random_state=config.RANDOM_STATE
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.33, stratify=y_temp, random_state=config.RANDOM_STATE
    )
    
    print(f"âœ… Train: {X_train.shape}")
    print(f"âœ… Val: {X_val.shape}")
    print(f"âœ… Test: {X_test.shape}")
    
    # 4. MODEL OLUÅTURMA
    print("\n" + "=" * 80)
    print("ğŸ—ï¸ MODEL OLUÅTURULUYOR")
    print("=" * 80)
    
    model = build_model(X_train.shape[1])
    
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy',
                 tf.keras.metrics.Precision(name='precision'),
                 tf.keras.metrics.Recall(name='recall'),
                 tf.keras.metrics.AUC(name='auc')]
    )
    
    print(f"âœ… Model hazÄ±r: {model.count_params():,} parametre")
    
    # 5. CALLBACKS
    callbacks = [
        ModelCheckpoint(
            os.path.join(config.OUTPUT_PATH, 'best_model.h5'),
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            verbose=0
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=7,
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    # 6. EÄÄ°TÄ°M
    print("\n" + "=" * 80)
    print(f"ğŸš€ EÄÄ°TÄ°M BAÅLIYOR ({config.EPOCHS} EPOCH)")
    print("=" * 80)
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=config.EPOCHS,
        batch_size=config.BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )
    
    # 7. TEST
    print("\n" + "=" * 80)
    print("ğŸ“Š TEST SONUÃ‡LARI")
    print("=" * 80)
    
    results = model.evaluate(X_test, y_test, verbose=0)
    
    print(f"\nâœ… Test Loss: {results[0]:.4f}")
    print(f"âœ… Test Accuracy: %{results[1]*100:.2f}")
    print(f"âœ… Test Precision: %{results[2]*100:.2f}")
    print(f"âœ… Test Recall: %{results[3]*100:.2f}")
    print(f"âœ… Test AUC: {results[4]:.4f}")
    
    # 8. KAYDET
    model.save(os.path.join(config.OUTPUT_PATH, 'final_model.h5'))
    
    # 9. RAPOR
    report_path = os.path.join(config.OUTPUT_PATH, 'training_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("NEURALCIPHER - EÄÄ°TÄ°M RAPORU\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"ğŸ“‚ Taranan Dosya: {scanner.stats['total_scanned']:,}\n")
        f.write(f"ğŸ“„ Bulunan CSV: {scanner.stats['csv_found']}\n")
        f.write(f"ğŸ“¥ YÃ¼klenen Dosya: {loader.loaded_count}\n")
        f.write(f"ğŸ“Š KullanÄ±lan Veri: {len(X):,} Ã¶rnek\n")
        f.write(f"ğŸ¯ Accuracy: %{results[1]*100:.2f}\n")
        f.write(f"ğŸ¯ AUC: {results[4]:.4f}\n")
    
    print(f"\nğŸ’¾ Model kaydedildi: {config.OUTPUT_PATH}")
    print(f"ğŸ“„ Rapor kaydedildi: {report_path}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ TAMAMLANDI!")
    print("=" * 80)
    print(f"\nğŸ“‚ Taranan Dosya: {scanner.stats['total_scanned']:,}")
    print(f"ğŸ“¥ YÃ¼klenen Dosya: {loader.loaded_count}")
    print(f"ğŸ“Š KullanÄ±lan Veri: {len(X):,} Ã¶rnek")
    print(f"ğŸ¯ Accuracy: %{results[1]*100:.2f}")
    print(f"ğŸ¯ AUC: {results[4]:.4f}")

# ============================================================
# Ã‡ALIÅTIR
# ============================================================
if __name__ == "__main__":
    main()
