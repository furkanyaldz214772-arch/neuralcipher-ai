#!/usr/bin/env python3
"""
NeuralCipher.ai - Production Model EÄŸitimi
UCI Parkinson veri seti ile Random Forest + Feature Engineering
"""
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix
)
import joblib
import json
from datetime import datetime

# KlasÃ¶rler
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
MODELS_DIR = BASE_DIR / "models"
MODELS_DIR.mkdir(exist_ok=True)

# Model versiyonu
MODEL_VERSION = "v1.0"
MODEL_NAME = f"neuralcipher_{MODEL_VERSION}"


def load_data():
    """UCI Parkinson veri setini yÃ¼kle"""
    print("ğŸ“‚ Veri yÃ¼kleniyor...")
    
    data_file = RAW_DIR / "parkinsons.data"
    if not data_file.exists():
        print("âŒ Veri seti bulunamadÄ±!")
        print("   Ã–nce ÅŸunu Ã§alÄ±ÅŸtÄ±rÄ±n: python scripts/download_data.py")
        sys.exit(1)
    
    df = pd.read_csv(data_file)
    print(f"âœ… Veri yÃ¼klendi: {len(df)} Ã¶rneklem")
    
    return df


def engineer_features(df):
    """Feature Engineering - Yeni Ã¶zellikler tÃ¼ret"""
    print("\nğŸ”§ Feature Engineering...")
    
    # Orijinal Ã¶zellikleri koru
    df_engineered = df.copy()
    
    # Jitter tÃ¼revleri
    df_engineered['jitter_ratio'] = df['MDVP:Jitter(%)'] / (df['MDVP:Jitter(Abs)'] + 1e-10)
    df_engineered['jitter_combined'] = (df['MDVP:Jitter(%)'] + df['MDVP:RAP'] + df['MDVP:PPQ']) / 3
    
    # Shimmer tÃ¼revleri
    df_engineered['shimmer_ratio'] = df['MDVP:Shimmer'] / (df['MDVP:Shimmer(dB)'] + 1e-10)
    df_engineered['shimmer_combined'] = (df['MDVP:Shimmer'] + df['Shimmer:APQ3'] + df['Shimmer:APQ5']) / 3
    
    # Frekans Ã¶zellikleri
    df_engineered['freq_range'] = df['MDVP:Fhi(Hz)'] - df['MDVP:Flo(Hz)']
    df_engineered['freq_variation'] = df['MDVP:Fhi(Hz)'] / (df['MDVP:Flo(Hz)'] + 1e-10)
    
    # Ses kalitesi bileÅŸik skoru
    df_engineered['voice_quality_score'] = (
        df['HNR'] * 0.4 +  # HNR yÃ¼ksek = iyi
        (1 / (df['NHR'] + 1e-10)) * 0.3 +  # NHR dÃ¼ÅŸÃ¼k = iyi
        (1 / (df['MDVP:Jitter(%)'] + 1e-10)) * 0.15 +  # Jitter dÃ¼ÅŸÃ¼k = iyi
        (1 / (df['MDVP:Shimmer'] + 1e-10)) * 0.15  # Shimmer dÃ¼ÅŸÃ¼k = iyi
    )
    
    # Nonlinear dynamics Ã¶zellikleri
    df_engineered['rpde_dfa_ratio'] = df['RPDE'] / (df['DFA'] + 1e-10)
    df_engineered['spread_ratio'] = df['spread1'] / (df['spread2'] + 1e-10)
    
    print(f"   Yeni Ã¶zellikler eklendi: {len(df_engineered.columns) - len(df.columns)} adet")
    
    return df_engineered


def prepare_features(df):
    """Ã–zellikleri hazÄ±rla"""
    print("\nğŸ¯ Ã–zellikler hazÄ±rlanÄ±yor...")
    
    # 'name' sÃ¼tununu Ã§Ä±kar
    if 'name' in df.columns:
        df = df.drop('name', axis=1)
    
    # Hedef deÄŸiÅŸken
    y = df['status']
    X = df.drop('status', axis=1)
    
    print(f"   Toplam Ã¶zellik sayÄ±sÄ±: {X.shape[1]}")
    print(f"   Parkinson hastalarÄ±: {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
    print(f"   SaÄŸlÄ±klÄ± bireyler: {len(y) - y.sum()} ({(len(y)-y.sum())/len(y)*100:.1f}%)")
    
    return X, y


def train_optimized_model(X_train, y_train):
    """Hiperparametre optimizasyonu ile model eÄŸit"""
    print("\nğŸ§  Model eÄŸitiliyor (Hiperparametre optimizasyonu)...")
    
    # Hiperparametre grid
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 15, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'max_features': ['sqrt', 'log2']
    }
    
    # Base model
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    
    # Grid Search
    print("   Grid Search baÅŸlatÄ±lÄ±yor...")
    grid_search = GridSearchCV(
        rf, param_grid, cv=5, scoring='roc_auc',
        n_jobs=-1, verbose=0
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"âœ… En iyi parametreler bulundu:")
    for param, value in grid_search.best_params_.items():
        print(f"   {param}: {value}")
    
    return grid_search.best_estimator_, grid_search.best_params_


def evaluate_model(model, X_test, y_test, X_train, y_train):
    """KapsamlÄ± model deÄŸerlendirmesi"""
    print("\nğŸ“Š Model deÄŸerlendiriliyor...")
    
    # Tahminler
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Metrikler
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    specificity = tn / (tn + fp)
    sensitivity = recall
    
    # Train accuracy (overfitting kontrolÃ¼)
    train_accuracy = model.score(X_train, y_train)
    
    # SonuÃ§larÄ± yazdÄ±r
    print("\n" + "=" * 60)
    print("ğŸ“ˆ PRODUCTION MODEL PERFORMANSI")
    print("=" * 60)
    print(f"\nğŸ¯ Test Seti Metrikleri:")
    print(f"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision:   {precision:.4f} ({precision*100:.2f}%)")
    print(f"   Recall:      {recall:.4f} ({recall*100:.2f}%)")
    print(f"   F1-Score:    {f1:.4f}")
    print(f"   AUC-ROC:     {auc:.4f}")
    
    print(f"\nğŸ”¬ Klinik Metrikler:")
    print(f"   Sensitivity: {sensitivity:.4f} ({sensitivity*100:.2f}%) - Hasta tespiti")
    print(f"   Specificity: {specificity:.4f} ({specificity*100:.2f}%) - SaÄŸlÄ±klÄ± tespiti")
    
    print(f"\nğŸ“Š Confusion Matrix:")
    print(f"   True Negatives:  {tn} (DoÄŸru saÄŸlÄ±klÄ±)")
    print(f"   False Positives: {fp} (YanlÄ±ÅŸ hasta)")
    print(f"   False Negatives: {fn} (KaÃ§an hasta)")
    print(f"   True Positives:  {tp} (DoÄŸru hasta)")
    
    print(f"\nğŸ” Overfitting KontrolÃ¼:")
    print(f"   Train Accuracy: {train_accuracy:.4f}")
    print(f"   Test Accuracy:  {accuracy:.4f}")
    print(f"   Fark:           {abs(train_accuracy - accuracy):.4f}")
    
    if abs(train_accuracy - accuracy) < 0.05:
        print("   âœ… Model iyi genelleÅŸtirilmiÅŸ (overfitting yok)")
    else:
        print("   âš ï¸  Overfitting riski var")
    
    # Ã–zellik Ã¶nemleri
    feature_importance = pd.DataFrame({
        'feature': X_test.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ” En Ã–nemli 10 Ã–zellik:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:30s}: {row['importance']:.4f}")
    
    # SonuÃ§larÄ± dict olarak dÃ¶ndÃ¼r
    results = {
        'model_version': MODEL_VERSION,
        'trained_at': datetime.now().isoformat(),
        'metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'auc_roc': float(auc),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity)
        },
        'confusion_matrix': {
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        },
        'overfitting_check': {
            'train_accuracy': float(train_accuracy),
            'test_accuracy': float(accuracy),
            'difference': float(abs(train_accuracy - accuracy))
        },
        'feature_importance': feature_importance.head(20).to_dict('records')
    }
    
    return results


def save_production_model(model, scaler, results, best_params, feature_names):
    """Production modeli kaydet"""
    print("\nğŸ’¾ Production model kaydediliyor...")
    
    # Model
    model_file = MODELS_DIR / f"{MODEL_NAME}.pkl"
    joblib.dump(model, model_file)
    print(f"âœ… Model: {model_file}")
    
    # Scaler
    scaler_file = MODELS_DIR / f"{MODEL_NAME}_scaler.pkl"
    joblib.dump(scaler, scaler_file)
    print(f"âœ… Scaler: {scaler_file}")
    
    # Feature names
    features_file = MODELS_DIR / f"{MODEL_NAME}_features.json"
    with open(features_file, 'w') as f:
        json.dump({'features': feature_names}, f, indent=2)
    print(f"âœ… Features: {features_file}")
    
    # Metadata
    metadata = {
        **results,
        'best_params': best_params,
        'feature_count': len(feature_names),
        'model_file': str(model_file.name),
        'scaler_file': str(scaler_file.name)
    }
    
    metadata_file = MODELS_DIR / f"{MODEL_NAME}_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"âœ… Metadata: {metadata_file}")
    
    print(f"\nğŸ“¦ Production model paketi hazÄ±r:")
    print(f"   {model_file.name}")
    print(f"   {scaler_file.name}")
    print(f"   {features_file.name}")
    print(f"   {metadata_file.name}")


def main():
    print("\n" + "=" * 60)
    print("ğŸ§¬ NEURALCIPHER.AI - PRODUCTION MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 60 + "\n")
    
    # 1. Veri yÃ¼kleme
    df = load_data()
    
    # 2. Feature Engineering
    df_engineered = engineer_features(df)
    
    # 3. Ã–zellik hazÄ±rlama
    X, y = prepare_features(df_engineered)
    
    # 4. Train-test split
    print("\nâœ‚ï¸  Veri bÃ¶lÃ¼nÃ¼yor (80% train, 20% test)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"   Train: {len(X_train)} Ã¶rneklem")
    print(f"   Test:  {len(X_test)} Ã¶rneklem")
    
    # 5. Normalizasyon
    print("\nğŸ“ Ã–zellikler normalize ediliyor...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # DataFrame'e geri Ã§evir
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
    
    # 6. Model eÄŸitimi (optimizasyon ile)
    model, best_params = train_optimized_model(X_train_scaled, y_train)
    
    # 7. DeÄŸerlendirme
    results = evaluate_model(model, X_test_scaled, y_test, X_train_scaled, y_train)
    
    # 8. Production model kaydetme
    save_production_model(model, scaler, results, best_params, X.columns.tolist())
    
    # 9. SonuÃ§ Ã¶zeti
    print("\n" + "=" * 60)
    print("âœ… PRODUCTION MODEL HAZIR!")
    print("=" * 60)
    
    accuracy = results['metrics']['accuracy']
    auc = results['metrics']['auc_roc']
    
    if accuracy >= 0.90 and auc >= 0.95:
        print("\nğŸ‰ MÃœKEMMEL! Model production iÃ§in hazÄ±r!")
    elif accuracy >= 0.85 and auc >= 0.90:
        print("\nâœ… Ä°YÄ°! Model hedeflere ulaÅŸtÄ±.")
    else:
        print("\nâš ï¸  Model iyileÅŸtirme gerektirebilir.")
    
    print(f"\nğŸ“Š Ã–zet:")
    print(f"   Accuracy: {accuracy*100:.2f}%")
    print(f"   AUC-ROC:  {auc:.4f}")
    print(f"   Sensitivity: {results['metrics']['sensitivity']*100:.2f}%")
    print(f"   Specificity: {results['metrics']['specificity']*100:.2f}%")
    
    print(f"\nğŸ“Œ Model dosyasÄ±: models/{MODEL_NAME}.pkl")
    print(f"ğŸ“Œ Backend entegrasyonu iÃ§in hazÄ±r!\n")


if __name__ == "__main__":
    main()
