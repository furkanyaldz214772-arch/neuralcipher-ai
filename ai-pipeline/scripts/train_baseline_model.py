#!/usr/bin/env python3
"""
Baseline Model EÄŸitimi - Random Forest
HÄ±zlÄ± prototip ve benchmark iÃ§in
"""
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
import joblib
import json

# KlasÃ¶rler
DATA_DIR = Path("data")
RAW_DIR = DATA_DIR / "raw"
MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True)

def load_data():
    """Veri setini yÃ¼kle"""
    print("ğŸ“‚ Veri yÃ¼kleniyor...")
    
    # UCI veri setini dene
    uci_file = RAW_DIR / "parkinsons.data"
    sample_file = RAW_DIR / "parkinsons_sample.data"
    
    if uci_file.exists():
        df = pd.read_csv(uci_file)
        print(f"âœ… UCI veri seti yÃ¼klendi: {len(df)} satÄ±r")
    elif sample_file.exists():
        df = pd.read_csv(sample_file)
        print(f"âœ… Ã–rnek veri seti yÃ¼klendi: {len(df)} satÄ±r")
    else:
        raise FileNotFoundError("Veri seti bulunamadÄ±! Ã–nce download_data.py Ã§alÄ±ÅŸtÄ±rÄ±n.")
    
    return df

def prepare_features(df):
    """Ã–zellikleri hazÄ±rla"""
    print("\nğŸ”§ Ã–zellikler hazÄ±rlanÄ±yor...")
    
    # 'name' sÃ¼tununu Ã§Ä±kar (ID)
    if 'name' in df.columns:
        df = df.drop('name', axis=1)
    
    # Ã–zellikler ve hedef
    X = df.drop('status', axis=1)
    y = df['status']
    
    print(f"   Ã–zellik sayÄ±sÄ±: {X.shape[1]}")
    print(f"   Pozitif Ã¶rnekler (Parkinson): {y.sum()}")
    print(f"   Negatif Ã¶rnekler (SaÄŸlÄ±klÄ±): {len(y) - y.sum()}")
    
    return X, y

def train_model(X_train, y_train):
    """Random Forest modelini eÄŸit"""
    print("\nğŸ§  Model eÄŸitiliyor...")
    
    # Random Forest parametreleri
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    # EÄŸitim
    model.fit(X_train, y_train)
    
    print("âœ… Model eÄŸitimi tamamlandÄ±!")
    
    return model

def evaluate_model(model, X_test, y_test, X_train, y_train):
    """Modeli deÄŸerlendir"""
    print("\nğŸ“Š Model deÄŸerlendiriliyor...")
    
    # Tahminler
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Metrikler
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    specificity = tn / (tn + fp)
    sensitivity = recall  # AynÄ± ÅŸey
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    # SonuÃ§larÄ± yazdÄ±r
    print("\n" + "=" * 60)
    print("ğŸ“ˆ MODEL PERFORMANSI")
    print("=" * 60)
    print(f"\nğŸ¯ Test Seti Metrikleri:")
    print(f"   Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision:   {precision:.4f} ({precision*100:.2f}%)")
    print(f"   Recall:      {recall:.4f} ({recall*100:.2f}%)")
    print(f"   F1-Score:    {f1:.4f}")
    print(f"   AUC-ROC:     {auc:.4f}")
    print(f"\nğŸ”¬ Klinik Metrikler:")
    print(f"   Sensitivity: {sensitivity:.4f} ({sensitivity*100:.2f}%) - Hasta tespiti")
    print(f"   Specificity: {specificity:.4f} ({specificity*100:.2f}%) - SaÄŸlÄ±klÄ± tespiti")
    print(f"\nğŸ“Š Confusion Matrix:")
    print(f"   True Negatives:  {tn}")
    print(f"   False Positives: {fp}")
    print(f"   False Negatives: {fn}")
    print(f"   True Positives:  {tp}")
    print(f"\nğŸ”„ Cross-Validation (5-fold):")
    print(f"   Mean Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    # Ã–zellik Ã¶nem sÄ±ralamasÄ±
    feature_importance = pd.DataFrame({
        'feature': X_test.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ” En Ã–nemli 10 Ã–zellik:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:20s}: {row['importance']:.4f}")
    
    # SonuÃ§larÄ± kaydet
    results = {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'auc_roc': float(auc),
        'sensitivity': float(sensitivity),
        'specificity': float(specificity),
        'confusion_matrix': {
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        },
        'cv_mean': float(cv_scores.mean()),
        'cv_std': float(cv_scores.std()),
        'feature_importance': feature_importance.to_dict('records')
    }
    
    return results

def save_model(model, scaler, results):
    """Modeli ve sonuÃ§larÄ± kaydet"""
    print("\nğŸ’¾ Model kaydediliyor...")
    
    # Model
    model_file = MODELS_DIR / "baseline_rf_model.pkl"
    joblib.dump(model, model_file)
    print(f"âœ… Model kaydedildi: {model_file}")
    
    # Scaler
    scaler_file = MODELS_DIR / "scaler.pkl"
    joblib.dump(scaler, scaler_file)
    print(f"âœ… Scaler kaydedildi: {scaler_file}")
    
    # SonuÃ§lar
    results_file = MODELS_DIR / "baseline_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ… SonuÃ§lar kaydedildi: {results_file}")

def main():
    print("\n" + "=" * 60)
    print("ğŸ§¬ NEURALCIPHER.AI - BASELINE MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 60 + "\n")
    
    # 1. Veri yÃ¼kleme
    df = load_data()
    
    # 2. Ã–zellik hazÄ±rlama
    X, y = prepare_features(df)
    
    # 3. Train-test split
    print("\nâœ‚ï¸  Veri bÃ¶lÃ¼nÃ¼yor (80% train, 20% test)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"   Train: {len(X_train)} Ã¶rneklem")
    print(f"   Test:  {len(X_test)} Ã¶rneklem")
    
    # 4. Normalizasyon
    print("\nğŸ“ Ã–zellikler normalize ediliyor...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # DataFrame'e geri Ã§evir (Ã¶zellik isimlerini korumak iÃ§in)
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
    
    # 5. Model eÄŸitimi
    model = train_model(X_train_scaled, y_train)
    
    # 6. DeÄŸerlendirme
    results = evaluate_model(model, X_test_scaled, y_test, X_train_scaled, y_train)
    
    # 7. Kaydetme
    save_model(model, scaler, results)
    
    # 8. SonuÃ§ Ã¶zeti
    print("\n" + "=" * 60)
    print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")
    print("=" * 60)
    
    if results['accuracy'] >= 0.85:
        print("\nğŸ‰ BAÅARILI! Model %85+ doÄŸruluk hedefine ulaÅŸtÄ±!")
    elif results['accuracy'] >= 0.80:
        print("\nâœ… Ä°YÄ°! Model %80+ doÄŸruluk elde etti.")
    else:
        print("\nâš ï¸  Model doÄŸruluÄŸu beklenenin altÄ±nda. Hiperparametre optimizasyonu gerekebilir.")
    
    print(f"\nğŸ“Œ Model dosyasÄ±: {MODELS_DIR / 'baseline_rf_model.pkl'}")
    print(f"ğŸ“Œ SonuÃ§lar: {MODELS_DIR / 'baseline_results.json'}")
    print("\nğŸ’¡ Sonraki adÄ±m: Neural Network modeli eÄŸitimi")
    print("   python scripts/train_neural_network.py\n")

if __name__ == "__main__":
    main()

