#!/usr/bin/env python3
"""
Yeni Dataset EÄŸitim Scripti
Herhangi bir CSV dataseti ile hÄ±zlÄ± model eÄŸitimi
"""

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import sys
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class NewDatasetTrainer:
    """Yeni dataset ile model eÄŸitimi"""
    
    def __init__(self, data_path):
        self.data_path = data_path
        self.model = None
        self.scaler = None
        self.feature_names = None
        
    def load_and_validate(self):
        """Veriyi yÃ¼kle ve doÄŸrula"""
        print("=" * 70)
        print("ğŸ“‚ VERÄ° YÃœKLEME VE DOÄRULAMA")
        print("=" * 70)
        
        # Veriyi yÃ¼kle
        try:
            df = pd.read_csv(self.data_path)
            print(f"\nâœ… Veri baÅŸarÄ±yla yÃ¼klendi: {self.data_path}")
        except Exception as e:
            print(f"\nâŒ HATA: Veri yÃ¼klenemedi!")
            print(f"   Hata: {e}")
            sys.exit(1)
        
        # Temel bilgiler
        print(f"\nğŸ“Š Dataset Bilgileri:")
        print(f"   SatÄ±r sayÄ±sÄ±: {len(df):,}")
        print(f"   Kolon sayÄ±sÄ±: {len(df.columns)}")
        print(f"   Boyut: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # KolonlarÄ± gÃ¶ster
        print(f"\nğŸ“‹ Kolonlar ({len(df.columns)} adet):")
        for i, col in enumerate(df.columns[:10], 1):
            print(f"   {i}. {col}")
        if len(df.columns) > 10:
            print(f"   ... ve {len(df.columns)-10} kolon daha")
        
        # Status kolonu kontrolÃ¼
        if 'status' not in df.columns:
            print(f"\nâš ï¸  'status' kolonu bulunamadÄ±!")
            print(f"   Mevcut kolonlar: {df.columns.tolist()}")
            
            # OlasÄ± etiket kolonlarÄ±nÄ± ara
            possible_labels = ['label', 'target', 'class', 'diagnosis', 'outcome']
            found = False
            for col in possible_labels:
                if col in df.columns:
                    print(f"\nâœ… '{col}' kolonu bulundu, 'status' olarak yeniden adlandÄ±rÄ±lÄ±yor...")
                    df = df.rename(columns={col: 'status'})
                    found = True
                    break
            
            if not found:
                print(f"\nâŒ Etiket kolonu bulunamadÄ±!")
                print(f"   LÃ¼tfen etiket kolonunu 'status' olarak adlandÄ±rÄ±n")
                sys.exit(1)
        
        # Status deÄŸerlerini kontrol et
        print(f"\nğŸ¯ Etiket DaÄŸÄ±lÄ±mÄ±:")
        status_counts = df['status'].value_counts()
        for label, count in status_counts.items():
            percentage = count / len(df) * 100
            label_name = "Parkinson" if label == 1 else "SaÄŸlÄ±klÄ±"
            print(f"   {label_name} ({label}): {count:,} (%{percentage:.1f})")
        
        # Eksik deÄŸer kontrolÃ¼
        missing = df.isnull().sum()
        if missing.sum() > 0:
            print(f"\nâš ï¸  Eksik deÄŸerler bulundu:")
            for col, count in missing[missing > 0].items():
                print(f"   {col}: {count} eksik")
            print(f"\nğŸ”§ Eksik deÄŸerler ortalama ile doldurulacak...")
            df = df.fillna(df.mean(numeric_only=True))
        else:
            print(f"\nâœ… Eksik deÄŸer yok")
        
        # Veri tiplerini kontrol et
        non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()
        if 'status' in non_numeric:
            non_numeric.remove('status')
        
        if non_numeric:
            print(f"\nâš ï¸  SayÄ±sal olmayan kolonlar bulundu:")
            for col in non_numeric:
                print(f"   {col}: {df[col].dtype}")
            print(f"\nğŸ”§ Bu kolonlar Ã§Ä±karÄ±lacak...")
            df = df.drop(columns=non_numeric)
        
        return df
    
    def prepare_data(self, df):
        """Veriyi hazÄ±rla"""
        print("\n" + "=" * 70)
        print("ğŸ”§ VERÄ° HAZIRLAMA")
        print("=" * 70)
        
        # Ã–zellikler ve etiketler
        X = df.drop('status', axis=1)
        y = df['status']
        
        # Ä°sim kolonu varsa Ã§Ä±kar
        if 'name' in X.columns:
            X = X.drop('name', axis=1)
            print(f"\nâœ… 'name' kolonu Ã§Ä±karÄ±ldÄ±")
        
        self.feature_names = X.columns.tolist()
        
        print(f"\nğŸ“Š HazÄ±rlanan Veri:")
        print(f"   Ã–zellik sayÄ±sÄ±: {X.shape[1]}")
        print(f"   Ã–rnek sayÄ±sÄ±: {X.shape[0]:,}")
        
        # Train/Test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\nâœ… Train/Test Split:")
        print(f"   Train: {len(X_train):,} Ã¶rnek")
        print(f"   Test: {len(X_test):,} Ã¶rnek")
        print(f"   Train Parkinson: {sum(y_train==1):,} (%{sum(y_train==1)/len(y_train)*100:.1f})")
        print(f"   Test Parkinson: {sum(y_test==1):,} (%{sum(y_test==1)/len(y_test)*100:.1f})")
        
        # Normalizasyon
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"\nâœ… Normalizasyon tamamlandÄ±")
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train_model(self, X_train, y_train):
        """Model eÄŸit"""
        print("\n" + "=" * 70)
        print("ğŸš€ MODEL EÄÄ°TÄ°MÄ°")
        print("=" * 70)
        
        print(f"\nâ³ XGBoost modeli eÄŸitiliyor...")
        print(f"   Parametreler:")
        print(f"   - n_estimators: 500")
        print(f"   - max_depth: 7")
        print(f"   - learning_rate: 0.1")
        
        self.model = XGBClassifier(
            n_estimators=500,
            max_depth=7,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss'
        )
        
        self.model.fit(X_train, y_train)
        
        print(f"\nâœ… Model eÄŸitimi tamamlandÄ±!")
        
    def evaluate_model(self, X_test, y_test):
        """Modeli deÄŸerlendir"""
        print("\n" + "=" * 70)
        print("ğŸ“Š MODEL DEÄERLENDÄ°RME")
        print("=" * 70)
        
        # Tahminler
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # DoÄŸruluk
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\nğŸ¯ Test DoÄŸruluÄŸu: %{accuracy*100:.2f}")
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nğŸ“ˆ Confusion Matrix:")
        print(f"   True Negatives (DoÄŸru SaÄŸlÄ±klÄ±):  {cm[0][0]}")
        print(f"   False Positives (YanlÄ±ÅŸ Parkinson): {cm[0][1]}")
        print(f"   False Negatives (YanlÄ±ÅŸ SaÄŸlÄ±klÄ±):  {cm[1][0]}")
        print(f"   True Positives (DoÄŸru Parkinson):  {cm[1][1]}")
        
        # Classification Report
        print(f"\nğŸ“‹ DetaylÄ± Rapor:")
        print(classification_report(y_test, y_pred, 
                                    target_names=['SaÄŸlÄ±klÄ±', 'Parkinson'],
                                    digits=4))
        
        # Ã–zellik Ã¶nem sÄ±ralamasÄ±
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” En Ã–nemli 10 Ã–zellik:")
        for i, row in feature_importance.head(10).iterrows():
            print(f"   {row['feature']:30s}: {row['importance']:.4f}")
        
        return accuracy
    
    def cross_validate(self, X, y):
        """Cross-validation"""
        print("\n" + "=" * 70)
        print("ğŸ”„ CROSS-VALIDATION (5-Fold)")
        print("=" * 70)
        
        print(f"\nâ³ Cross-validation yapÄ±lÄ±yor...")
        
        cv_scores = cross_val_score(
            self.model, X, y, cv=5, scoring='accuracy', n_jobs=-1
        )
        
        print(f"\nğŸ“Š Fold SonuÃ§larÄ±:")
        for i, score in enumerate(cv_scores, 1):
            print(f"   Fold {i}: %{score*100:.2f}")
        
        print(f"\nâœ… Ortalama CV DoÄŸruluÄŸu: %{cv_scores.mean()*100:.2f} (Â±{cv_scores.std()*100:.2f})")
        
        return cv_scores
    
    def save_model(self):
        """Modeli kaydet"""
        print("\n" + "=" * 70)
        print("ğŸ’¾ MODEL KAYDETME")
        print("=" * 70)
        
        # KlasÃ¶r oluÅŸtur
        output_dir = Path("models/new_dataset")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Model kaydet
        model_path = output_dir / "model.pkl"
        joblib.dump(self.model, model_path)
        
        # Scaler kaydet
        scaler_path = output_dir / "scaler.pkl"
        joblib.dump(self.scaler, scaler_path)
        
        # Feature names kaydet
        features_path = output_dir / "features.txt"
        with open(features_path, 'w') as f:
            f.write('\n'.join(self.feature_names))
        
        print(f"\nâœ… Model kaydedildi:")
        print(f"   Model: {model_path}")
        print(f"   Scaler: {scaler_path}")
        print(f"   Features: {features_path}")
        
        return model_path
    
    def run_full_pipeline(self):
        """Tam pipeline Ã§alÄ±ÅŸtÄ±r"""
        print("\n" + "=" * 70)
        print("ğŸ¯ YENÄ° DATASET EÄÄ°TÄ°M PÄ°PELINE")
        print("=" * 70)
        
        # 1. Veri yÃ¼kle ve doÄŸrula
        df = self.load_and_validate()
        
        # 2. Veriyi hazÄ±rla
        X_train, X_test, y_train, y_test = self.prepare_data(df)
        
        # 3. Model eÄŸit
        self.train_model(X_train, y_train)
        
        # 4. DeÄŸerlendir
        accuracy = self.evaluate_model(X_test, y_test)
        
        # 5. Cross-validation
        X = df.drop('status', axis=1)
        if 'name' in X.columns:
            X = X.drop('name', axis=1)
        y = df['status']
        X_scaled = self.scaler.transform(X)
        cv_scores = self.cross_validate(X_scaled, y)
        
        # 6. Model kaydet
        model_path = self.save_model()
        
        # Ã–zet
        print("\n" + "=" * 70)
        print("ğŸ‰ EÄÄ°TÄ°M TAMAMLANDI!")
        print("=" * 70)
        print(f"\nğŸ“Š SonuÃ§ Ã–zeti:")
        print(f"   Test DoÄŸruluÄŸu: %{accuracy*100:.2f}")
        print(f"   CV DoÄŸruluÄŸu: %{cv_scores.mean()*100:.2f} (Â±{cv_scores.std()*100:.2f})")
        print(f"   Model: {model_path}")
        print(f"\nâœ… Model kullanÄ±ma hazÄ±r!")

def main():
    """Ana fonksiyon"""
    if len(sys.argv) < 2:
        print("\nâŒ HATA: Veri dosyasÄ± belirtilmedi!")
        print("\nKullanÄ±m:")
        print("  python train_new_dataset.py <data_path>")
        print("\nÃ–rnekler:")
        print("  python train_new_dataset.py data/yeni_veri.csv")
        print("  python train_new_dataset.py C:\\Users\\...\\dataset.csv")
        sys.exit(1)
    
    data_path = sys.argv[1]
    
    # Dosya var mÄ± kontrol et
    if not Path(data_path).exists():
        print(f"\nâŒ HATA: Dosya bulunamadÄ±: {data_path}")
        sys.exit(1)
    
    # Trainer oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r
    trainer = NewDatasetTrainer(data_path)
    trainer.run_full_pipeline()

if __name__ == "__main__":
    main()
