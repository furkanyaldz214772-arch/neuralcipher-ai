#!/usr/bin/env python3
"""
TÃœM VERÄ°YÄ° KULLAN - MAKSIMUM MODEL
Oxford (195) + Telemonitoring (5,875) + Sentetik (5,000) = 11,070 Ã–RNEK!
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
from pathlib import Path
import json
from datetime import datetime

class MaximumDataTrainer:
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.data_dir = self.base_dir / "data" / "raw"
        self.models_dir = self.base_dir / "models"
        self.models_dir.mkdir(exist_ok=True)
        
        self.model = None
        self.scaler = StandardScaler()
        
    def load_all_data(self):
        """TÃœM VERÄ°YÄ° YÃœK - HÄ°Ã‡BÄ°R ÅžEYÄ° ATMA!"""
        print("="*60)
        print("ðŸ“Š TÃœM VERÄ°YÄ° YÃœKLÃœYORUM - HÄ°Ã‡BÄ°R ÅžEY SÄ°LÄ°NMEYECEK!")
        print("="*60)
        
        all_data = []
        
        # 1. Oxford Parkinson's (195 Ã¶rnek, 22 Ã¶zellik)
        print("\n1ï¸âƒ£ Oxford Parkinson's Dataset")
        oxford_file = self.data_dir / "parkinsons.data"
        if oxford_file.exists():
            df_oxford = pd.read_csv(oxford_file)
            if 'name' in df_oxford.columns:
                df_oxford = df_oxford.drop('name', axis=1)
            
            print(f"   âœ… {len(df_oxford)} Ã¶rnek yÃ¼klendi")
            print(f"   ðŸ“Š Ã–zellik: {len(df_oxford.columns)-1}")
            print(f"   ðŸ”´ Parkinson: {sum(df_oxford['status']==1)}")
            print(f"   ðŸŸ¢ SaÄŸlÄ±klÄ±: {sum(df_oxford['status']==0)}")
            all_data.append(('Oxford', df_oxford))
        
        # 2. Telemonitoring (5,875 Ã¶rnek, 16 Ã¶zellik)
        print("\n2ï¸âƒ£ Telemonitoring Dataset")
        tele_file = self.data_dir / "parkinsons_updrs.data"
        if tele_file.exists():
            df_tele = pd.read_csv(tele_file)
            
            # Gereksiz sÃ¼tunlarÄ± Ã§Ä±kar
            drop_cols = ['subject#', 'age', 'sex', 'test_time', 'motor_UPDRS', 'total_UPDRS']
            df_tele = df_tele.drop([col for col in drop_cols if col in df_tele.columns], axis=1)
            
            # Status sÃ¼tunu ekle (Telemonitoring hepsi Parkinson hastasÄ±)
            df_tele['status'] = 1
            
            print(f"   âœ… {len(df_tele)} Ã¶rnek yÃ¼klendi")
            print(f"   ðŸ“Š Ã–zellik: {len(df_tele.columns)-1}")
            print(f"   âš ï¸  Ã–zellik sayÄ±sÄ± farklÄ±! Oxford: 22, Telemonitoring: {len(df_tele.columns)-1}")
            
            # Ortak Ã¶zellikleri bul
            oxford_features = set(all_data[0][1].columns) - {'status'}
            tele_features = set(df_tele.columns) - {'status'}
            common_features = oxford_features & tele_features
            
            print(f"   ðŸ”— Ortak Ã¶zellik: {len(common_features)}")
            
            # Sadece ortak Ã¶zellikleri kullan
            common_features_list = list(common_features) + ['status']
            df_tele = df_tele[common_features_list]
            
            print(f"   âœ… {len(df_tele)} Ã¶rnek eklendi (ortak Ã¶zelliklerle)")
            all_data.append(('Telemonitoring', df_tele))
        
        # 3. Sentetik Veri (5,000 Ã¶rnek, 22 Ã¶zellik)
        print("\n3ï¸âƒ£ Sentetik Dataset")
        synthetic_file = self.data_dir / "synthetic_parkinsons_5000.csv"
        if synthetic_file.exists():
            df_synthetic = pd.read_csv(synthetic_file)
            if 'name' in df_synthetic.columns:
                df_synthetic = df_synthetic.drop('name', axis=1)
            
            print(f"   âœ… {len(df_synthetic)} Ã¶rnek yÃ¼klendi")
            print(f"   ðŸ“Š Ã–zellik: {len(df_synthetic.columns)-1}")
            print(f"   ðŸ”´ Parkinson: {sum(df_synthetic['status']==1)}")
            print(f"   ðŸŸ¢ SaÄŸlÄ±klÄ±: {sum(df_synthetic['status']==0)}")
            all_data.append(('Synthetic', df_synthetic))
        
        # Ortak Ã¶zellikleri bul
        print("\nðŸ”— Ortak Ã–zellikleri Buluyorum...")
        all_features = [set(df.columns) - {'status'} for name, df in all_data]
        common_features = set.intersection(*all_features)
        
        print(f"   âœ… Ortak Ã¶zellik sayÄ±sÄ±: {len(common_features)}")
        print(f"   ðŸ“‹ Ã–zellikler: {sorted(common_features)[:5]}... (ilk 5)")
        
        # TÃ¼m veriyi birleÅŸtir (sadece ortak Ã¶zelliklerle)
        print("\nðŸ”— TÃ¼m Veriyi BirleÅŸtiriyorum...")
        common_features_list = list(common_features) + ['status']
        
        combined_dfs = []
        for name, df in all_data:
            df_filtered = df[common_features_list]
            combined_dfs.append(df_filtered)
            print(f"   âœ… {name}: {len(df_filtered)} Ã¶rnek eklendi")
        
        df_combined = pd.concat(combined_dfs, ignore_index=True)
        
        # AyrÄ±ÅŸtÄ±r
        X = df_combined.drop('status', axis=1).values
        y = df_combined['status'].values
        
        print(f"\nâœ… TOPLAM VERÄ°:")
        print(f"   ðŸ“Š Ã–rnek: {len(X):,}")
        print(f"   ðŸ“Š Ã–zellik: {X.shape[1]}")
        print(f"   ðŸ”´ Parkinson: {np.sum(y == 1):,} ({np.mean(y)*100:.1f}%)")
        print(f"   ðŸŸ¢ SaÄŸlÄ±klÄ±: {np.sum(y == 0):,} ({(1-np.mean(y))*100:.1f}%)")
        
        return X, y, len(common_features)
    
    def prepare_data(self, X, y, test_size=0.2):
        """Veriyi hazÄ±rla"""
        print("\n" + "="*60)
        print("ðŸ”§ VERÄ°YÄ° HAZIRLIYORUM")
        print("="*60)
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Normalize
        X_train = self.scaler.fit_transform(X_train)
        X_test = self.scaler.transform(X_test)
        
        print(f"\nâœ… HazÄ±r!")
        print(f"   ðŸ“Š Train: {X_train.shape[0]:,} Ã¶rnek")
        print(f"   ðŸ“Š Test: {X_test.shape[0]:,} Ã¶rnek")
        
        return X_train, X_test, y_train, y_test
    
    def train_model(self, X_train, y_train):
        """Model eÄŸit - GÃœÃ‡LÃœ AYARLAR!"""
        print("\n" + "="*60)
        print("ðŸš€ MODEL EÄžÄ°TÄ°MÄ° BAÅžLIYOR")
        print("="*60)
        
        print("\nðŸŒ² Random Forest - MAKSIMUM GÃœÃ‡!")
        print("   ðŸŒ³ AÄŸaÃ§: 300 (daha fazla veri = daha fazla aÄŸaÃ§!)")
        print("   ðŸ“ Max Depth: 25")
        print("   ðŸ”¢ Min Samples Split: 5")
        
        self.model = RandomForestClassifier(
            n_estimators=300,  # Daha fazla!
            max_depth=25,      # Daha derin!
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        print("\nðŸ”„ EÄŸitim baÅŸladÄ±...")
        self.model.fit(X_train, y_train)
        
        print("\nâœ… EÄŸitim tamamlandÄ±!")
        
        return self.model
    
    def evaluate_model(self, X_train, y_train, X_test, y_test):
        """Model deÄŸerlendir"""
        print("\n" + "="*60)
        print("ðŸ“Š MODEL DEÄžERLENDÄ°RME")
        print("="*60)
        
        # Train accuracy
        train_pred = self.model.predict(X_train)
        train_acc = accuracy_score(y_train, train_pred)
        
        # Test predictions
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # Metrics
        test_acc = accuracy_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Sensitivity and Specificity
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        # Classification report
        report = classification_report(y_test, y_pred, output_dict=True)
        
        print(f"\nðŸŽ¯ SONUÃ‡LAR:")
        print(f"\n   Train Accuracy:  {train_acc*100:.2f}%")
        print(f"   Test Accuracy:   {test_acc*100:.2f}%")
        print(f"   ROC-AUC:         {roc_auc:.4f} ({roc_auc*100:.2f}%)")
        print(f"   Sensitivity:     {sensitivity*100:.2f}%")
        print(f"   Specificity:     {specificity*100:.2f}%")
        print(f"   F1-Score:        {report['1']['f1-score']*100:.2f}%")
        
        print(f"\nðŸ“ˆ Confusion Matrix:")
        print(f"   TN: {tn:,}  FP: {fp:,}")
        print(f"   FN: {fn:,}  TP: {tp:,}")
        
        return {
            'train_accuracy': float(train_acc),
            'test_accuracy': float(test_acc),
            'roc_auc': float(roc_auc),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity),
            'f1_score': float(report['1']['f1-score']),
            'precision': float(report['1']['precision']),
            'recall': float(report['1']['recall']),
            'confusion_matrix': {
                'tn': int(tn), 'fp': int(fp),
                'fn': int(fn), 'tp': int(tp)
            }
        }
    
    def cross_validate(self, X, y, cv=5):
        """Cross-validation"""
        print("\n" + "="*60)
        print("ðŸ”„ CROSS-VALIDATION")
        print("="*60)
        
        X_scaled = self.scaler.fit_transform(X)
        
        cv_scores = cross_val_score(
            self.model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1
        )
        
        print(f"\nâœ… CV SonuÃ§larÄ±:")
        print(f"   Skorlar: {[f'{s*100:.2f}%' for s in cv_scores]}")
        print(f"   Ortalama: {cv_scores.mean()*100:.2f}%")
        print(f"   Std: Â±{cv_scores.std()*100:.2f}%")
        
        return {
            'scores': cv_scores.tolist(),
            'mean': float(cv_scores.mean()),
            'std': float(cv_scores.std())
        }
    
    def save_model(self, results, cv_results, n_features):
        """Modeli kaydet"""
        print("\n" + "="*60)
        print("ðŸ’¾ MODEL KAYDEDILIYOR")
        print("="*60)
        
        version = "v6.0"  # Yeni versiyon!
        model_name = f"neuralcipher_{version}"
        
        # Save model
        model_path = self.models_dir / f"{model_name}.pkl"
        joblib.dump(self.model, model_path)
        print(f"\nâœ… Model: {model_path}")
        
        # Save scaler
        scaler_path = self.models_dir / f"{model_name}_scaler.pkl"
        joblib.dump(self.scaler, scaler_path)
        print(f"âœ… Scaler: {scaler_path}")
        
        # Save metadata
        metadata = {
            'version': version,
            'training_date': datetime.now().isoformat(),
            'training_samples': 11070,
            'datasets': [
                {'name': 'Oxford Parkinson\'s', 'samples': 195},
                {'name': 'Telemonitoring', 'samples': 5875},
                {'name': 'Synthetic', 'samples': 5000}
            ],
            'features': n_features,
            'algorithm': 'Random Forest',
            'hyperparameters': {
                'n_estimators': 300,
                'max_depth': 25,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            },
            'performance': results,
            'cross_validation': cv_results
        }
        
        metadata_path = self.models_dir / f"{model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"âœ… Metadata: {metadata_path}")
        
        return model_path
    
    def run(self):
        """Tam pipeline"""
        print("\n" + "="*60)
        print("ðŸš€ MAKSIMUM VERÄ° - MAKSIMUM MODEL!")
        print("="*60)
        print(f"\nðŸ’ª TÃœM VERÄ°YÄ° KULLANIYORUZ!")
        print(f"   Oxford: 195")
        print(f"   Telemonitoring: 5,875")
        print(f"   Sentetik: 5,000")
        print(f"   TOPLAM: 11,070 Ã–RNEK!")
        
        # Load
        X, y, n_features = self.load_all_data()
        
        # Prepare
        X_train, X_test, y_train, y_test = self.prepare_data(X, y)
        
        # Train
        self.train_model(X_train, y_train)
        
        # Evaluate
        results = self.evaluate_model(X_train, y_train, X_test, y_test)
        
        # Cross-validate
        cv_results = self.cross_validate(X, y)
        
        # Save
        model_path = self.save_model(results, cv_results, n_features)
        
        print("\n" + "="*60)
        print("ðŸŽ‰ TAMAMLANDI!")
        print("="*60)
        print(f"\nðŸ“Š Final SonuÃ§lar:")
        print(f"   Veri: 11,070 Ã¶rnek")
        print(f"   Ã–zellik: {n_features}")
        print(f"   Test Accuracy: {results['test_accuracy']*100:.2f}%")
        print(f"   ROC-AUC: {results['roc_auc']*100:.2f}%")
        print(f"   CV Mean: {cv_results['mean']*100:.2f}%")
        print(f"\nðŸ’¾ Model: {model_path}")
        print(f"\nðŸš€ Backend'i gÃ¼ncelle: MODEL_VERSION = 'v6.0'")
        
        return results

if __name__ == "__main__":
    trainer = MaximumDataTrainer()
    trainer.run()
