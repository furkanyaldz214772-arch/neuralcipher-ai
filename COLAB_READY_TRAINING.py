"""
üöÄ GOOGLE COLAB - TEK KOMUT Eƒûƒ∞Tƒ∞M
===================================
Bu kodu direkt Google Colab'a yapƒ±≈ütƒ±r ve √ßalƒ±≈ütƒ±r!
"""

# ============================================================
# ADIM 1: KURULUM
# ============================================================
print("üîß Installing dependencies...")
import subprocess
import sys

try:
    import librosa
except:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "librosa"])
    print("‚úÖ librosa installed")

# ============================================================
# ADIM 2: GOOGLE DRIVE MOUNT
# ============================================================
print("\nüìÇ Mounting Google Drive...")
try:
    from google.colab import drive
    import os
    if not os.path.exists('/content/drive'):
        drive.mount('/content/drive')
        print("‚úÖ Drive mounted")
    else:
        print("‚úÖ Drive already mounted")
except:
    print("‚ö†Ô∏è Not in Colab, skipping drive mount")

# ============================================================
# ADIM 3: Eƒûƒ∞Tƒ∞M KODU
# ============================================================
print("\n" + "=" * 60)
print("üöÄ NEURALCIPHER - GER√áEK VERƒ∞ Eƒûƒ∞Tƒ∞Mƒ∞")
print("=" * 60)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import glob
import os

# Config
class Config:
    DATA_PATH = "/content/drive/MyDrive/Veriler/"
    OUTPUT_PATH = "/content/drive/MyDrive/NeuralCipher_Output/"
    BATCH_SIZE = 32
    EPOCHS = 50
    LEARNING_RATE = 0.001
    RANDOM_STATE = 42

config = Config()
os.makedirs(config.OUTPUT_PATH, exist_ok=True)

print(f"\n‚öôÔ∏è Configuration:")
print(f"üìÇ Data: {config.DATA_PATH}")
print(f"üíæ Output: {config.OUTPUT_PATH}")
print(f"üîÑ Epochs: {config.EPOCHS}")

# GPU Setup
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    print(f"‚úÖ GPU: {len(gpus)} device(s)")
else:
    print("‚ö†Ô∏è No GPU, using CPU")

# Data Loader
class DataLoader:
    def __init__(self, data_path):
        self.data_path = data_path
        self.scaler = StandardScaler()
    
    def load_csv_data(self):
        print("\nüîç Searching for CSV files...")
        csv_files = glob.glob(os.path.join(self.data_path, "**/*.csv"), recursive=True)
        
        # √ñncelikli dosyalar
        priority_files = [
            'parkinsons.data',
            'parkinsons.csv', 
            'parkinsons_updrs.csv',
            'Parkinson_Sample_100.csv',
            'Parkinson_Sample_500.csv'
        ]
        
        all_data = []
        for priority in priority_files:
            for csv_file in csv_files:
                if priority in os.path.basename(csv_file):
                    try:
                        df = pd.read_csv(csv_file)
                        if len(df) > 0:
                            all_data.append(df)
                            print(f"‚úÖ {os.path.basename(csv_file)}: {len(df)} rows")
                    except:
                        continue
        
        if all_data:
            combined = pd.concat(all_data, ignore_index=True)
            print(f"\n‚úÖ Total: {len(combined)} rows")
            return combined
        return None
    
    def create_sample_data(self, n=1000):
        print("\nüîß Creating sample data...")
        X = np.random.randn(n, 22).astype(np.float32)
        y = np.random.randint(0, 2, n)
        print(f"‚úÖ Sample: {X.shape}")
        return X, y
    
    def load_all(self):
        print("\n" + "=" * 60)
        print("üì• LOADING DATA")
        print("=" * 60)
        
        csv_data = self.load_csv_data()
        
        if csv_data is not None:
            # Status column'u bul
            status_col = None
            for col in ['status', 'Status', 'label', 'Label', 'class', 'Class']:
                if col in csv_data.columns:
                    status_col = col
                    break
            
            if status_col:
                X = csv_data.drop([status_col, 'name', 'Name'], axis=1, errors='ignore').values
                y = csv_data[status_col].values
                
                X = self.scaler.fit_transform(X)
                
                print(f"\n‚úÖ Data loaded!")
                print(f"üìä Shape: {X.shape}")
                print(f"üè∑Ô∏è PD: {np.sum(y)}, HC: {len(y) - np.sum(y)}")
                return X, y
        
        print("\n‚ö†Ô∏è No CSV data found, using sample data")
        return self.create_sample_data()

# Model
def build_model(input_dim):
    inputs = Input(shape=(input_dim,))
    
    x = Dense(256, activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    
    outputs = Dense(1, activation='sigmoid')(x)
    
    return models.Model(inputs=inputs, outputs=outputs)

# Main Training
def main():
    # Load
    loader = DataLoader(config.DATA_PATH)
    X, y = loader.load_all()
    
    # Split
    print("\n" + "=" * 60)
    print("‚úÇÔ∏è SPLITTING DATA")
    print("=" * 60)
    
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, stratify=y, random_state=config.RANDOM_STATE
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.33, stratify=y_temp, random_state=config.RANDOM_STATE
    )
    
    print(f"‚úÖ Train: {X_train.shape}")
    print(f"‚úÖ Val: {X_val.shape}")
    print(f"‚úÖ Test: {X_test.shape}")
    
    # Build
    print("\n" + "=" * 60)
    print("üèóÔ∏è BUILDING MODEL")
    print("=" * 60)
    
    model = build_model(X_train.shape[1])
    
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', 
                 tf.keras.metrics.Precision(),
                 tf.keras.metrics.Recall(),
                 tf.keras.metrics.AUC()]
    )
    
    print(f"‚úÖ Parameters: {model.count_params():,}")
    
    # Callbacks
    callbacks = [
        ModelCheckpoint(
            os.path.join(config.OUTPUT_PATH, 'best_model.h5'),
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    # Train
    print("\n" + "=" * 60)
    print("üöÄ TRAINING")
    print("=" * 60)
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=config.EPOCHS,
        batch_size=config.BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )
    
    # Evaluate
    print("\n" + "=" * 60)
    print("üìä RESULTS")
    print("=" * 60)
    
    results = model.evaluate(X_test, y_test, verbose=0)
    
    print(f"\n‚úÖ Test Loss: {results[0]:.4f}")
    print(f"‚úÖ Test Accuracy: {results[1]:.4f}")
    print(f"‚úÖ Test Precision: {results[2]:.4f}")
    print(f"‚úÖ Test Recall: {results[3]:.4f}")
    print(f"‚úÖ Test AUC: {results[4]:.4f}")
    
    # Save
    model.save(os.path.join(config.OUTPUT_PATH, 'final_model.h5'))
    print(f"\nüíæ Saved: {config.OUTPUT_PATH}")
    
    print("\n" + "=" * 60)
    print("üéâ COMPLETE!")
    print("=" * 60)

# Run
if __name__ == "__main__":
    main()
